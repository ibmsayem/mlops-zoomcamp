#### If you give an MLOps engineer a job...

* Could you just setup this pipeline to train this model?
* could you setup logging?
* could you do it everyday?
* could you retry it after it fails?
* Could you send me an email if it works?
* could you visualize the dependencies?
* could you add caching?
* could you add some collaborators to run ad hoc?
* 
To solve all of this, prefect comes in, which allows to orchestrate and observe Python workflows at a high-level scale, like parallelization among multiple machines
synchronization etc.
